{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.3 디스크립터 추출기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 키포인트는 영상의 특징이 있는 픽셀의 좌표와 그 주변 픽셀과의 관계에 대한 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징 디스크립터\n",
    "# 키 포인트 주변 픽셀을 일정한 크기의 블록으로 나누어 각 블록에 속한 픽셀의 그레이디언트 히스토그램\n",
    "# 을 계산한 것으로, 키 포인트 주위의 밝기, 색상, 방향, 크기 등의 정보를 표현한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keypoints, descriptors = detector.compute(image, keypoints[, descriptors]):\n",
    "# 키 포인트를 전달하면 특징 디스크립터를 계산해서 반환\n",
    "# keypoints, descriptors = detector.detectAndcompute(image, mask[, descriptors, useprovidedkeypoints]): 키 포인트 검출과 특징 디스크립터 계산을 한번에 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIFT\n",
    "# 이미지 피라미드를 이용해서 크기 변화에 따른 특징 검출의 문제를 해결한 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detector = cv2.xfeatures2d.SIFT_create([, nfeatures[, n0ctaveLayers[, \n",
    "#                      contrastThreshold[, edgeThreshold[, sigma]]]]])\n",
    "# nfeatures: 검출 최대 특징 수\n",
    "# n0ctvaveLayers: 이미지 피라미드에 사용할 계층 수\n",
    "# contrastThreshold: 필터링할 빈약한 특징 문턱 값\n",
    "# edgeThreshold: 필터링할 엣지 문턱 값\n",
    "# sigma: 이미지 피라미드 0계층에서 사용할 가우시안 필터의 시그마 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'xfeatures2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-33cdec1ee2dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# SIFT 추출기 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIFT_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# 키 포인트 검출과 서술자 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'xfeatures2d'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('C:\\\\Users\\\\user\\\\AI_vision\\\\data\\\\img\\\\house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SIFT 추출기 생성\n",
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "# 키 포인트 검출과 서술자 계산\n",
    "keypoints, descriptor = sift.detectAndCompute(gray, None)\n",
    "print('keypoint:',len(keypoints), 'descriptor:', descriptor.shape)\n",
    "print(descriptor)\n",
    "\n",
    "# 키 포인트 그리기\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "# 결과 출력\n",
    "cv2.imshow('SIFT', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SURF(speeded pu robust features)\n",
    "# 필터의 커널 크기를 바꾸는 방식으로 성능을 개선한 알고리즘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2.cv2' has no attribute 'xfeatures2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c6a1bb46538c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# SURF 추출기 생성 ( 경계:1000, 피라미드:3, 서술자확장:True, 방향적용:True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0msurf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSURF_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m# 키 포인트 검출 및 서술자 계산\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mkeypoints\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetectAndCompute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2.cv2' has no attribute 'xfeatures2d'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img = cv2.imread('C:\\\\Users\\\\user\\\\AI_vision\\\\data\\\\img\\\\house.jpg')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# SURF 추출기 생성 ( 경계:1000, 피라미드:3, 서술자확장:True, 방향적용:True)\n",
    "surf = cv2.xfeatures2d.SURF_create(1000, 3, True, True)\n",
    "# 키 포인트 검출 및 서술자 계산\n",
    "keypoints, desc = surf.detectAndCompute(gray, None)\n",
    "print(desc.shape, desc)\n",
    "# 키포인트 이미지에 그리기\n",
    "img_draw = cv2.drawKeypoints(img, keypoints, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "cv2.imshow('SURF', img_draw)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ORB: 특징 검출을 지원하지 않는 디스크립터 추출기인 BRIEF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특징 매칭(feature matching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8.4 좋은 매칭점 찾기\n",
    "# 쓸모없는 매칭점은 버리고, 좋은 매칭점만 골라내는 작업이 필요.\n",
    "# 좋은 매칭점을 골라냈더니, 남아있는 매칭점이 없다면 두 영상간의 관계가 없다고 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches:18/127, min:24.00, max:78.00, thresh:34.80\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('C:\\\\Users\\\\user\\\\AI_vision\\\\data\\\\img\\\\taekwonv1.jpg')\n",
    "img2 = cv2.imread('C:\\\\Users\\\\user\\\\AI_vision\\\\data\\\\img\\\\figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY) #\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB로 디스크립터 추출 ---①\n",
    "detector = cv2.ORB_create()\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "# BF-Hamming으로 매칭 ---②\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = matcher.match(desc1, desc2)\n",
    "\n",
    "# 매칭 결과를 거리기준 오름차순으로 정렬 ---③\n",
    "matches = sorted(matches, key=lambda x:x.distance)\n",
    "# 최소 거리 값과 최대 거리 값 확보 ---④\n",
    "min_dist, max_dist = matches[0].distance, matches[-1].distance\n",
    "# 최소 거리의 15% 지점을 임계점으로 설정 ---⑤\n",
    "ratio = 0.2\n",
    "good_thresh = (max_dist - min_dist) * ratio + min_dist\n",
    "# 임계점 보다 작은 매칭점만 좋은 매칭점으로 분류 ---⑥\n",
    "good_matches = [m for m in matches if m.distance < good_thresh]\n",
    "print('matches:%d/%d, min:%.2f, max:%.2f, thresh:%.2f' \\\n",
    "        %(len(good_matches),len(matches), min_dist, max_dist, good_thresh))\n",
    "# 좋은 매칭점만 그리기 ---⑦\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "# 결과 출력\n",
    "cv2.imshow('Good Match', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matches:23/500\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = cv2.imread('C:\\\\Users\\\\user\\\\AI_vision\\\\data\\\\img\\\\taekwonv1.jpg')\n",
    "img2 = cv2.imread('C:\\\\Users\\\\user\\\\AI_vision\\\\data\\\\img\\\\figures.jpg')\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# ORB로 서술자 추출 ---①\n",
    "detector = cv2.ORB_create()\n",
    "kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "# BF-Hamming 생성 ---②\n",
    "matcher = cv2.BFMatcher(cv2.NORM_HAMMING2)\n",
    "# knnMatch, k=2 ---③\n",
    "matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "\n",
    "# 첫번재 이웃의 거리가 두 번째 이웃 거리의 75% 이내인 것만 추출---⑤\n",
    "ratio = 0.75\n",
    "good_matches = [first for first,second in matches \\\n",
    "                    if first.distance < second.distance * ratio]\n",
    "print('matches:%d/%d' %(len(good_matches),len(matches)))\n",
    "\n",
    "# 좋은 매칭만 그리기\n",
    "res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                    flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "# 결과 출력                    \n",
    "cv2.imshow('Matching', res)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 왜 2만 되지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 매칭 영역 원근 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 카메라로 객체 매핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "\n",
    "img1 = None\n",
    "win_name = 'Camera Matching'\n",
    "MIN_MATCH = 10\n",
    "# ORB 검출기 생성  ---①\n",
    "detector = cv2.ORB_create(1000)\n",
    "# Flann 추출기 생성 ---②\n",
    "FLANN_INDEX_LSH = 6\n",
    "index_params= dict(algorithm = FLANN_INDEX_LSH,\n",
    "                   table_number = 6,\n",
    "                   key_size = 12,\n",
    "                   multi_probe_level = 1)\n",
    "search_params=dict(checks=32)\n",
    "matcher = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "# 카메라 캡쳐 연결 및 프레임 크기 축소 ---③\n",
    "cap = cv2.VideoCapture(0)              \n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "\n",
    "while cap.isOpened():       \n",
    "    ret, frame = cap.read() \n",
    "    if img1 is None:  # 등록된 이미지 없음, 카메라 바이패스\n",
    "        res = frame\n",
    "    else:             # 등록된 이미지 있는 경우, 매칭 시작\n",
    "        img2 = frame\n",
    "        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "        # 키포인트와 디스크립터 추출\n",
    "        kp1, desc1 = detector.detectAndCompute(gray1, None)\n",
    "        kp2, desc2 = detector.detectAndCompute(gray2, None)\n",
    "        # k=2로 knnMatch\n",
    "        matches = matcher.knnMatch(desc1, desc2, 2)\n",
    "        # 이웃 거리의 75%로 좋은 매칭점 추출---②\n",
    "        ratio = 0.75\n",
    "        good_matches = [m[0] for m in matches \\\n",
    "                if len(m) == 2 and m[0].distance < m[1].distance * ratio]\n",
    "        print('good matches:%d/%d' %(len(good_matches),len(matches)))\n",
    "        # 모든 매칭점 그리지 못하게 마스크를 0으로 채움\n",
    "        matchesMask = np.zeros(len(good_matches)).tolist()\n",
    "        # 좋은 매칭점 최소 갯수 이상 인 경우\n",
    "        if len(good_matches) > MIN_MATCH: \n",
    "            # 좋은 매칭점으로 원본과 대상 영상의 좌표 구하기 ---③\n",
    "            src_pts = np.float32([ kp1[m.queryIdx].pt for m in good_matches ])\n",
    "            dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good_matches ])\n",
    "            # 원근 변환 행렬 구하기 ---⑤\n",
    "            mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "            accuracy=float(mask.sum()) / mask.size\n",
    "            print(\"accuracy: %d/%d(%.2f%%)\"% (mask.sum(), mask.size, accuracy))\n",
    "            if mask.sum() > MIN_MATCH:  # 정상치 매칭점 최소 갯수 이상 인 경우\n",
    "                # 이상점 매칭점만 그리게 마스크 설정\n",
    "                matchesMask = mask.ravel().tolist()\n",
    "                # 원본 영상 좌표로 원근 변환 후 영역 표시  ---⑦\n",
    "                h,w, = img1.shape[:2]\n",
    "                pts = np.float32([ [[0,0]],[[0,h-1]],[[w-1,h-1]],[[w-1,0]] ])\n",
    "                dst = cv2.perspectiveTransform(pts,mtrx)\n",
    "                img2 = cv2.polylines(img2,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n",
    "        # 마스크로 매칭점 그리기 ---⑨\n",
    "        res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \\\n",
    "                            matchesMask=matchesMask,\n",
    "                            flags=cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)\n",
    "    # 결과 출력\n",
    "    cv2.imshow(win_name, res)\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == 27:    # Esc, 종료\n",
    "            break          \n",
    "    elif key == ord(' '): # 스페이스바를 누르면 ROI로 img1 설정\n",
    "        x,y,w,h = cv2.selectROI(win_name, frame, False)\n",
    "        if w and h:\n",
    "            img1 = frame[y:y+h, x:x+w]\n",
    "else:\n",
    "    print(\"can't open camera.\")\n",
    "cap.release()                          \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
